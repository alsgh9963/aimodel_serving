version: "3"

services:
  inference_server:
    image: nvcr.io/nvidia/tritonserver:21.04-py3
    restart: always
    volumes:
      - ./model:/model
    ports:
      - 80:8000
      - 8001:8001
    command: ["tritonserver", "--model-repository=/model"]
